{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Radit1234/Boiwala/blob/master/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rONPSCy2dKV3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNRFE1JQdbUQ",
        "outputId": "a74698a1-b6e8-437a-f891-58ae25d67fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYxdp1Njdbf1",
        "outputId": "393a01d4-9888-48a2-e823-499d341f07bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/drive/My Drive/cats_and _dogs_small': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "#directory where our dataset present\n",
        "!ls \"/content/drive/My Drive/cats_and _dogs_small\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iOBufO0eDjN",
        "outputId": "58ca5a57-e0f7-48d3-8dfe-0bbef623446f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.DS_Store', 'test', 'validation', 'train']\n",
            "['.DS_Store', 'dogs', 'cats']\n"
          ]
        }
      ],
      "source": [
        "data_dir = \"/content/drive/MyDrive/cats_and_dogs_small\"\n",
        "\n",
        "print(os.listdir(data_dir)) #folders inside dataset directory\n",
        "\n",
        "classes = os.listdir(data_dir + \"/train\")\n",
        "print(classes) #all 5 classes we want to classify by our model\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2npkKXideDlp",
        "outputId": "65e0f535-5439-4940-9090-229fe8ea7633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of training examples for cats images: 1000\n",
            "['cat.279.jpg', 'cat.240.jpg', 'cat.465.jpg', 'cat.307.jpg', 'cat.489.jpg']\n"
          ]
        }
      ],
      "source": [
        "cats_files = os.listdir(data_dir + \"/train/cats\")\n",
        "print('No. of training examples for cats images:', len(cats_files))\n",
        "print(cats_files[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfRgN98-eDof",
        "outputId": "f027b05d-f0aa-4824-9afc-ee1301c061c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of training examples for dogs images: 1030\n",
            "['dog.839.jpg', 'dog.56.jpg', 'dog.607.jpg', 'dog.358.jpg', 'dog.313.jpg']\n"
          ]
        }
      ],
      "source": [
        "dogs_files = os.listdir(data_dir + \"/train/dogs\")\n",
        "print('No. of training examples for dogs images:', len(dogs_files))\n",
        "print(dogs_files[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6H1WcWu97es",
        "outputId": "55160ab2-4da8-4dd4-c497-93d2685030ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.DS_Store', 'test', 'validation', 'train']\n",
            "['.DS_Store', 'cats', 'dogs']\n"
          ]
        }
      ],
      "source": [
        "data_dir = \"/content/drive/MyDrive/cats_and_dogs_small\"\n",
        "\n",
        "print(os.listdir(data_dir)) #folders inside dataset directory\n",
        "\n",
        "val_ds = os.listdir(data_dir + \"/validation\")\n",
        "print(val_ds) #all 5 classes we want to classify by our model\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GiaZof1Ixwp_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from torch.utils import data as D\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cqTSi1i_xwtD"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "validation_ratio = 0.1\n",
        "random_seed = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5JJ5AqvaTHW",
        "outputId": "6b4c573e-e099-476c-dc9f-b6eb3be18988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB_5rzub_AKI",
        "outputId": "fdffdbfa-9289-40ec-f25f-745dc6ff5f32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.DS_Store', 'test', 'validation', 'train']\n",
            "['.DS_Store', 'dogs', 'cats']\n"
          ]
        }
      ],
      "source": [
        "data_dir = \"/content/drive/MyDrive/cats_and_dogs_small\"\n",
        "\n",
        "print(os.listdir(data_dir)) #folders inside dataset directory\n",
        "\n",
        "classes = os.listdir(data_dir + \"/train\")\n",
        "print(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1lhKyWBaTQ7",
        "outputId": "e5b7e734-32cf-4fd2-988d-1ee88036cbde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of training examples for cats images: 1000\n",
            "['cat.279.jpg', 'cat.240.jpg', 'cat.465.jpg', 'cat.307.jpg', 'cat.489.jpg']\n"
          ]
        }
      ],
      "source": [
        "cats_files = os.listdir(data_dir + \"/train/cats\")\n",
        "print('No. of training examples for cats images:', len(cats_files))\n",
        "print(cats_files[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgyFX-2qaTT5",
        "outputId": "4c59a69a-4b79-4756-8c7a-3c01a33c9cd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of training examples for dogs images: 1030\n",
            "['dog.839.jpg', 'dog.56.jpg', 'dog.607.jpg', 'dog.358.jpg', 'dog.313.jpg']\n"
          ]
        }
      ],
      "source": [
        "dogs_files = os.listdir(data_dir + \"/train/dogs\")\n",
        "print('No. of training examples for dogs images:', len(dogs_files))\n",
        "print(dogs_files[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jzj2siOaTXG",
        "outputId": "aa05408f-0e98-459d-db51-656f29465431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.DS_Store', 'test', 'validation', 'train']\n",
            "['.DS_Store', 'dogs', 'cats']\n"
          ]
        }
      ],
      "source": [
        "data_dir = \"/content/drive/MyDrive/cats_and_dogs_small\"\n",
        "\n",
        "print(os.listdir(data_dir)) #folders inside dataset directory\n",
        "\n",
        "classes = os.listdir(data_dir + \"/train\")\n",
        "print(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jrr0Fd2OaTaG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jX5fK87YaXjo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zw9eVRqhaXoG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WalUaWvQxw0_"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "\n",
        "train_transformations = transforms.Compose([\n",
        "        transforms.Resize((299,299)),\n",
        "        transforms.RandomCrop(299, padding=38),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
        "\n",
        "validation_transformations = transforms.Compose([\n",
        "        transforms.Resize((299,299)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
        "\n",
        "\n",
        "test_transformations = transforms.Compose([\n",
        "        transforms.Resize((299,299)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
        "\n",
        "#applt the train and test transformations\n",
        "training_dataset = ImageFolder(data_dir + \"/train\", transform = train_transformations) \n",
        " \n",
        "testing_dataset= ImageFolder(\"/content/drive/MyDrive/cats_and_dogs_small/test\", transform= test_transformations)\n",
        "validation_dataset= ImageFolder(\"/content/drive/MyDrive/cats_and_dogs_small/validation\", transform= validation_transformations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zMVMcYyZ1Wb"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "37AIDcwhANAp"
      },
      "outputs": [],
      "source": [
        "# num_train = len(training_dataset)\n",
        "# indices = list(range(num_train))\n",
        "# split = int(np.floor(validation_ratio * num_train))\n",
        "\n",
        "# np.random.seed(random_seed)\n",
        "# np.random.shuffle(indices)\n",
        "\n",
        "# train_idx, valid_idx = indices[split:], indices[:split]\n",
        "# train_sampler = SubsetRandomSampler(train_idx)\n",
        "# valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "import torch\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    training_dataset, batch_size=batch_size\n",
        ")\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    validation_dataset, batch_size=batch_size\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    testing_dataset, batch_size=batch_size\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrFCqi9HZ0Dd",
        "outputId": "4761e858-49d2-4740-b4e1-a43f291634c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n"
          ]
        }
      ],
      "source": [
        "for i, j in test_loader:\n",
        "  print(type(i))\n",
        "  \n",
        "  break "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-1hBVzzcZGKJ"
      },
      "outputs": [],
      "source": [
        "classess = ( 'cat','dog')\n",
        "\n",
        "initial_lr = 0.045\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OvOWD2pEZGSL"
      },
      "outputs": [],
      "source": [
        "class depthwise_separable_conv(nn.Module):\n",
        "    def __init__(self, nin, nout, kernel_size, padding, bias=False):\n",
        "        super(depthwise_separable_conv, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(nin, nin, kernel_size=kernel_size, padding=padding, groups=nin, bias=bias)\n",
        "        self.pointwise = nn.Conv2d(nin, nout, kernel_size=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.depthwise(x)\n",
        "        out = self.pointwise(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rjYQAmjsZMy4"
      },
      "outputs": [],
      "source": [
        "class Xception(nn.Module):\n",
        "    def __init__(self, input_channel, num_classes=10):\n",
        "        super(Xception, self).__init__()\n",
        "        \n",
        "        # Entry Flow\n",
        "        self.entry_flow_1 = nn.Sequential(\n",
        "            nn.Conv2d(input_channel, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(True),\n",
        "            \n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        \n",
        "        self.entry_flow_2 = nn.Sequential(\n",
        "            depthwise_separable_conv(64, 128, 3, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            \n",
        "            depthwise_separable_conv(128, 128, 3, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "        \n",
        "        self.entry_flow_2_residual = nn.Conv2d(64, 128, kernel_size=1, stride=2, padding=0)\n",
        "        \n",
        "        self.entry_flow_3 = nn.Sequential(\n",
        "            nn.ReLU(True),\n",
        "            depthwise_separable_conv(128, 256, 3, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            \n",
        "            nn.ReLU(True),\n",
        "            depthwise_separable_conv(256, 256, 3, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            \n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "        \n",
        "        self.entry_flow_3_residual = nn.Conv2d(128, 256, kernel_size=1, stride=2, padding=0)\n",
        "        \n",
        "        self.entry_flow_4 = nn.Sequential(\n",
        "            nn.ReLU(True),\n",
        "            depthwise_separable_conv(256, 728, 3, 1),\n",
        "            nn.BatchNorm2d(728),\n",
        "            \n",
        "            nn.ReLU(True),\n",
        "            depthwise_separable_conv(728, 728, 3, 1),\n",
        "            nn.BatchNorm2d(728),\n",
        "            \n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "        \n",
        "        self.entry_flow_4_residual = nn.Conv2d(256, 728, kernel_size=1, stride=2, padding=0)\n",
        "        \n",
        "        # Middle Flow\n",
        "        self.middle_flow = nn.Sequential(\n",
        "            nn.ReLU(True),\n",
        "            depthwise_separable_conv(728, 728, 3, 1),\n",
        "            nn.BatchNorm2d(728),\n",
        "            \n",
        "            nn.ReLU(True),\n",
        "            depthwise_separable_conv(728, 728, 3, 1),\n",
        "            nn.BatchNorm2d(728),\n",
        "            \n",
        "            nn.ReLU(True),\n",
        "            depthwise_separable_conv(728, 728, 3, 1),\n",
        "            nn.BatchNorm2d(728)\n",
        "        )\n",
        "        \n",
        "        # Exit Flow\n",
        "        self.exit_flow_1 = nn.Sequential(\n",
        "            nn.ReLU(True),\n",
        "            depthwise_separable_conv(728, 728, 3, 1),\n",
        "            nn.BatchNorm2d(728),\n",
        "            \n",
        "            nn.ReLU(True),\n",
        "            depthwise_separable_conv(728, 1024, 3, 1),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            \n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "        self.exit_flow_1_residual = nn.Conv2d(728, 1024, kernel_size=1, stride=2, padding=0)\n",
        "        self.exit_flow_2 = nn.Sequential(\n",
        "            depthwise_separable_conv(1024, 1536, 3, 1),\n",
        "            nn.BatchNorm2d(1536),\n",
        "            nn.ReLU(True),\n",
        "            \n",
        "            depthwise_separable_conv(1536, 2048, 3, 1),\n",
        "            nn.BatchNorm2d(2048),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        \n",
        "        self.linear = nn.Linear(2048, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        entry_out1 = self.entry_flow_1(x)\n",
        "        entry_out2 = self.entry_flow_2(entry_out1) + self.entry_flow_2_residual(entry_out1)\n",
        "        entry_out3 = self.entry_flow_3(entry_out2) + self.entry_flow_3_residual(entry_out2)\n",
        "        entry_out = self.entry_flow_4(entry_out3) + self.entry_flow_4_residual(entry_out3)\n",
        "        \n",
        "        middle_out = self.middle_flow(entry_out) + entry_out\n",
        "        \n",
        "        for i in range(7):\n",
        "          middle_out = self.middle_flow(middle_out) + middle_out\n",
        "\n",
        "        exit_out1 = self.exit_flow_1(middle_out) + self.exit_flow_1_residual(middle_out)\n",
        "        exit_out2 = self.exit_flow_2(exit_out1)\n",
        "\n",
        "        exit_avg_pool = F.adaptive_avg_pool2d(exit_out2, (1, 1))                \n",
        "        exit_avg_pool_flat = exit_avg_pool.view(exit_avg_pool.size(0), -1)\n",
        "\n",
        "        output = self.linear(exit_avg_pool_flat)\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SlBAN9E4ZM3t"
      },
      "outputs": [],
      "source": [
        "net = Xception(3, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfSCXiYgZM9h",
        "outputId": "cce7296e-d342-42b7-f9e6-1f6712314cfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1NhSIVZZNFg",
        "outputId": "f90deb8c-eac8-432c-aa6b-1ec0825b4933"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Xception(\n",
              "  (entry_flow_1): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "  )\n",
              "  (entry_flow_2): Sequential(\n",
              "    (0): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "      (pointwise): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
              "      (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (entry_flow_2_residual): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
              "  (entry_flow_3): Sequential(\n",
              "    (0): ReLU(inplace=True)\n",
              "    (1): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
              "      (pointwise): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "      (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (entry_flow_3_residual): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
              "  (entry_flow_4): Sequential(\n",
              "    (0): ReLU(inplace=True)\n",
              "    (1): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "      (pointwise): Conv2d(256, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "      (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (entry_flow_4_residual): Conv2d(256, 728, kernel_size=(1, 1), stride=(2, 2))\n",
              "  (middle_flow): Sequential(\n",
              "    (0): ReLU(inplace=True)\n",
              "    (1): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "      (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "      (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "      (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (exit_flow_1): Sequential(\n",
              "    (0): ReLU(inplace=True)\n",
              "    (1): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "      (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "      (pointwise): Conv2d(728, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (exit_flow_1_residual): Conv2d(728, 1024, kernel_size=(1, 1), stride=(2, 2))\n",
              "  (exit_flow_2): Sequential(\n",
              "    (0): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
              "      (pointwise): Conv2d(1024, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
              "      (pointwise): Conv2d(1536, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (4): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "  )\n",
              "  (linear): Linear(in_features=2048, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hD45z18JpG6s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5gry-z0zxe0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cl4aJo3FZQ__"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=initial_lr, momentum=0.9)\n",
        "\n",
        "for epoch in range(100):  \n",
        "    if epoch == 0:\n",
        "        lr = initial_lr\n",
        "    elif epoch % 2 == 0 and epoch != 0:\n",
        "        lr *= 0.94\n",
        "        optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
        "    \n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \"\"\"\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        show_period = 250\n",
        "        if i % show_period == show_period-1:    # print every \"show_period\" mini-batches\n",
        "            print('[%d, %5d] loss: %.7f' %\n",
        "                  (epoch + 1, i + 1, running_loss / show_period))\n",
        "            running_loss = 0.0\n",
        "        \"\"\"\n",
        "        \n",
        "    #validation part\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, data in enumerate(valid_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = net(inputs)\n",
        "        \n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "    print('[%d epoch] Accuracy of the network on the validation images: %d %%' % \n",
        "          (epoch, 100 * correct / total)\n",
        "         )\n",
        "\n",
        "print('Finished Training')\n",
        "    \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "outputs = net(images)\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join('%2s' % classes[predicted[j]]\n",
        "                              for j in range(2)))"
      ],
      "metadata": {
        "id": "wUMp5b4JXSNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FzuGefPevo1"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaFAFWmrAOAt"
      },
      "outputs": [],
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "                \n",
        "        for i in range(labels.shape[0]):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9Q0dS60AOHN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kww1a2DZ6jbh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3QEqGs_6jii"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVKqI5c06jnF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}